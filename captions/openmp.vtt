WEBVTT

00:00:00.000 --> 00:00:01.530 align:middle line:90%


00:00:01.530 --> 00:00:04.740 align:middle line:84%
Modern CPUs have many
cores, each of them

00:00:04.740 --> 00:00:08.790 align:middle line:84%
capable of executing an
independent instruction thread.

00:00:08.790 --> 00:00:12.220 align:middle line:84%
However, for your code to
take advantage of that,

00:00:12.220 --> 00:00:15.630 align:middle line:84%
it has to be designed
in order to do so.

00:00:15.630 --> 00:00:17.580 align:middle line:84%
For scientific
computing, you can

00:00:17.580 --> 00:00:21.780 align:middle line:84%
use OpenMP, which is a very
convenient way of exploiting

00:00:21.780 --> 00:00:23.640 align:middle line:90%
parallelism.

00:00:23.640 --> 00:00:25.860 align:middle line:84%
You annotate your
code with directives,

00:00:25.860 --> 00:00:29.880 align:middle line:84%
and the compiler will
automatically generate code

00:00:29.880 --> 00:00:33.060 align:middle line:84%
to spawn threads, and
to divide the workload

00:00:33.060 --> 00:00:35.160 align:middle line:90%
among those threads.

00:00:35.160 --> 00:00:38.700 align:middle line:84%
It is the de facto standard
for shared memory programming

00:00:38.700 --> 00:00:41.700 align:middle line:90%
in scientific computing.

00:00:41.700 --> 00:00:45.630 align:middle line:84%
It's a very extensive topic,
and here, we will barely

00:00:45.630 --> 00:00:46.960 align:middle line:90%
scratch the surface.

00:00:46.960 --> 00:00:49.860 align:middle line:84%
But PRACE offers quite a
number of courses dealing

00:00:49.860 --> 00:00:52.500 align:middle line:90%
with OpenMP in much more depth.

00:00:52.500 --> 00:00:58.110 align:middle line:90%


00:00:58.110 --> 00:01:01.140 align:middle line:84%
We consider an
application to compute pi.

00:01:01.140 --> 00:01:04.860 align:middle line:84%
The way this is done
is by throwing darts

00:01:04.860 --> 00:01:06.810 align:middle line:90%
at a square board.

00:01:06.810 --> 00:01:10.740 align:middle line:84%
In that square board,
a circle is inscribed,

00:01:10.740 --> 00:01:17.320 align:middle line:84%
and those darts that land
in the circle, those count.

00:01:17.320 --> 00:01:21.130 align:middle line:84%
So we do that by counting
the number of successes,

00:01:21.130 --> 00:01:26.540 align:middle line:84%
and then we have to loop from
1 to the total number of tries,

00:01:26.540 --> 00:01:29.380 align:middle line:84%
so the number of darts
which are going to thow.

00:01:29.380 --> 00:01:33.610 align:middle line:84%
We determine the x- and
the y-coordinate just by

00:01:33.610 --> 00:01:36.910 align:middle line:84%
way of a random
number, and we compute

00:01:36.910 --> 00:01:41.320 align:middle line:84%
whether those
particular coordinates

00:01:41.320 --> 00:01:45.860 align:middle line:90%
are inside the circle or not.

00:01:45.860 --> 00:01:50.870 align:middle line:84%
If that's the case, we increment
the number of successes by one.

00:01:50.870 --> 00:01:54.100 align:middle line:84%
And then, finally,
we can compute pi

00:01:54.100 --> 00:01:57.850 align:middle line:84%
by multiplying the
number of successes by 4

00:01:57.850 --> 00:02:01.810 align:middle line:84%
and dividing it by
the number of tries--

00:02:01.810 --> 00:02:04.460 align:middle line:90%
so quite straightforward.

00:02:04.460 --> 00:02:08.449 align:middle line:84%
The thing to note is that
the iterations of the

00:02:08.449 --> 00:02:14.650 align:middle line:84%
do loop are almost
independent, not quite,

00:02:14.650 --> 00:02:19.900 align:middle line:84%
because in each iteration, we
update number of successes.

00:02:19.900 --> 00:02:22.000 align:middle line:84%
And that is common
to each iteration,

00:02:22.000 --> 00:02:25.810 align:middle line:84%
but still, it's
almost independent.

00:02:25.810 --> 00:02:28.270 align:middle line:84%
And that's something
we can exploit in order

00:02:28.270 --> 00:02:32.560 align:middle line:84%
to do this in parallel,
because we could divide

00:02:32.560 --> 00:02:37.600 align:middle line:84%
the iterations into various
sequences of iterations, which

00:02:37.600 --> 00:02:40.570 align:middle line:84%
we can do in parallel
if we keep track

00:02:40.570 --> 00:02:44.320 align:middle line:84%
of the number of
successes in a proper way.

00:02:44.320 --> 00:02:46.480 align:middle line:90%
We can compile and run the code.

00:02:46.480 --> 00:02:51.010 align:middle line:84%
And doing so, 400
million iterations

00:02:51.010 --> 00:02:54.310 align:middle line:84%
gives us 5.5 seconds
approximately

00:02:54.310 --> 00:02:55.940 align:middle line:90%
as execution time.

00:02:55.940 --> 00:02:59.380 align:middle line:84%
So you can see, we get pi up
to five significant digits

00:02:59.380 --> 00:03:01.510 align:middle line:90%
in the sampled pi.

00:03:01.510 --> 00:03:05.770 align:middle line:84%
The execution of the program
unit will start in line 11,

00:03:05.770 --> 00:03:10.810 align:middle line:84%
and that just sets the initial
value for number of success.

00:03:10.810 --> 00:03:14.530 align:middle line:84%
Then we have our first
OpenMP directive,

00:03:14.530 --> 00:03:18.550 align:middle line:90%
and that's indicated by !$omp.

00:03:18.550 --> 00:03:22.450 align:middle line:84%
So that indicates it is an
OpenMP directive, something

00:03:22.450 --> 00:03:25.450 align:middle line:84%
the compiler has to
take into account.

00:03:25.450 --> 00:03:27.770 align:middle line:84%
And in this case,
it's a parallel.

00:03:27.770 --> 00:03:29.980 align:middle line:90%
So this opens a parallel region.

00:03:29.980 --> 00:03:34.030 align:middle line:84%
That means we had a
single thread executing.

00:03:34.030 --> 00:03:35.740 align:middle line:84%
When we open a
parallel region, we

00:03:35.740 --> 00:03:39.370 align:middle line:84%
get multiple threads all
doing the same thing,

00:03:39.370 --> 00:03:41.810 align:middle line:90%
unless we differentiate.

00:03:41.810 --> 00:03:46.900 align:middle line:84%
So what we need to
specify is which variables

00:03:46.900 --> 00:03:49.270 align:middle line:90%
are private to the threads--

00:03:49.270 --> 00:03:51.100 align:middle line:84%
that means the threads
have their own copy

00:03:51.100 --> 00:03:53.650 align:middle line:84%
of that variable--
and which are shared,

00:03:53.650 --> 00:03:57.700 align:middle line:84%
so which variables can be
accessed by all the threads

00:03:57.700 --> 00:04:00.600 align:middle line:90%
simultaneously.

00:04:00.600 --> 00:04:02.160 align:middle line:84%
The private variables
in this case

00:04:02.160 --> 00:04:05.880 align:middle line:84%
are x and y, the
coordinates and num_threads

00:04:05.880 --> 00:04:09.600 align:middle line:84%
and thread_num, which we'll
explain later, what they do.

00:04:09.600 --> 00:04:11.400 align:middle line:90%
So those are private.

00:04:11.400 --> 00:04:15.360 align:middle line:90%
The shared one is nr_success.

00:04:15.360 --> 00:04:18.450 align:middle line:84%
And we declare
default none, which

00:04:18.450 --> 00:04:21.390 align:middle line:84%
means that for
each variable that

00:04:21.390 --> 00:04:23.670 align:middle line:84%
occurs in our
parallel region, we

00:04:23.670 --> 00:04:26.850 align:middle line:84%
have to make a decision
whether it's private or shared.

00:04:26.850 --> 00:04:29.390 align:middle line:90%


00:04:29.390 --> 00:04:34.760 align:middle line:84%
In the next two lines, we
call some OpenMP functions,

00:04:34.760 --> 00:04:38.120 align:middle line:84%
omp_get_num_threads,
and omp_get_thread_num.

00:04:38.120 --> 00:04:41.270 align:middle line:84%
So the first one
gives the number

00:04:41.270 --> 00:04:45.710 align:middle line:84%
of threads which are active
in this parallel region.

00:04:45.710 --> 00:04:49.100 align:middle line:84%
And the second one,
omp_get_thread_num,

00:04:49.100 --> 00:04:53.240 align:middle line:84%
returns the thread
ID, the thread number

00:04:53.240 --> 00:04:56.870 align:middle line:90%
of each specific thread.

00:04:56.870 --> 00:05:00.465 align:middle line:84%
So num_threads will be
the same for all threads.

00:05:00.465 --> 00:05:04.130 align:middle line:84%
thread_num will be
individual for each thread,

00:05:04.130 --> 00:05:09.380 align:middle line:84%
starting from 0 to the
number of threads minus 1.

00:05:09.380 --> 00:05:11.510 align:middle line:84%
And we can use
thread_num in order

00:05:11.510 --> 00:05:14.370 align:middle line:84%
to control the flow
of our application.

00:05:14.370 --> 00:05:18.350 align:middle line:84%
So in this case, for the first
thread, with thread number 0,

00:05:18.350 --> 00:05:21.500 align:middle line:84%
we print the total number
of threads which are

00:05:21.500 --> 00:05:25.000 align:middle line:90%
active in this parallel region.

00:05:25.000 --> 00:05:29.980 align:middle line:84%
Next, we have our do loop,
and we have a work sharing

00:05:29.980 --> 00:05:32.980 align:middle line:90%
directive, !$omp do.

00:05:32.980 --> 00:05:38.800 align:middle line:84%
So this is going to
divide the iterations, one

00:05:38.800 --> 00:05:42.820 align:middle line:84%
to number of tries, over
the threads which are

00:05:42.820 --> 00:05:44.920 align:middle line:90%
active in this parallel region.

00:05:44.920 --> 00:05:49.660 align:middle line:84%
So, now each thread gets some
portion of the iterations

00:05:49.660 --> 00:05:54.830 align:middle line:84%
to compute, independent
of the others.

00:05:54.830 --> 00:05:59.020 align:middle line:84%
There is a reduction clause
here because, as we already

00:05:59.020 --> 00:06:03.310 align:middle line:84%
mentioned, nr_success
is a bit of a problem.

00:06:03.310 --> 00:06:04.930 align:middle line:90%
It's a shared variable.

00:06:04.930 --> 00:06:10.810 align:middle line:84%
It is updated by each and
every thread in each iteration.

00:06:10.810 --> 00:06:14.390 align:middle line:84%
But it's only used to
accumulate information,

00:06:14.390 --> 00:06:17.200 align:middle line:90%
so just accumulate counts.

00:06:17.200 --> 00:06:19.720 align:middle line:84%
And hence, we can
specify that that's

00:06:19.720 --> 00:06:25.120 align:middle line:84%
the case by declaring a
reduction clause with addition

00:06:25.120 --> 00:06:28.790 align:middle line:90%
operator for nr_success.

00:06:28.790 --> 00:06:31.750 align:middle line:84%
So special code is
generated in order

00:06:31.750 --> 00:06:34.780 align:middle line:84%
to ensure that
although nr_success

00:06:34.780 --> 00:06:37.660 align:middle line:84%
is updated from
multiple threads,

00:06:37.660 --> 00:06:39.530 align:middle line:84%
that will not
cause any problems.

00:06:39.530 --> 00:06:44.480 align:middle line:84%
It will not result
in a race condition.

00:06:44.480 --> 00:06:48.100 align:middle line:84%
When we are done with
the workshare do,

00:06:48.100 --> 00:06:51.760 align:middle line:90%
we end it with !$omp end do.

00:06:51.760 --> 00:06:54.800 align:middle line:84%
Then we're done with all the
work which happens in parallel.

00:06:54.800 --> 00:06:59.350 align:middle line:84%
So we close the parallel
region with !$omp end parallel.

00:06:59.350 --> 00:07:02.260 align:middle line:84%
So we started with
a single thread.

00:07:02.260 --> 00:07:06.850 align:middle line:84%
We got multiple threads by
opening the parallel region.

00:07:06.850 --> 00:07:10.450 align:middle line:84%
As soon as we close it, we
continue with a single thread.

00:07:10.450 --> 00:07:13.240 align:middle line:84%
And that single thread is
going to print the result

00:07:13.240 --> 00:07:16.750 align:middle line:84%
and going to print some
additional information.

00:07:16.750 --> 00:07:19.910 align:middle line:84%
And that's the end
of the programme.

00:07:19.910 --> 00:07:22.940 align:middle line:84%
To compile the
application using OpenMP,

00:07:22.940 --> 00:07:28.330 align:middle line:84%
we use the -fopenmp
compiler flag.

00:07:28.330 --> 00:07:30.640 align:middle line:84%
When we run the
application and time it,

00:07:30.640 --> 00:07:33.790 align:middle line:84%
you see that we get
the computation done

00:07:33.790 --> 00:07:36.050 align:middle line:84%
in less than a
second in this case.

00:07:36.050 --> 00:07:40.690 align:middle line:84%
And you can also see it's
running with 12 threads.

00:07:40.690 --> 00:07:42.820 align:middle line:84%
The result is very
similar to the one

00:07:42.820 --> 00:07:46.090 align:middle line:90%
of the serial application.

00:07:46.090 --> 00:07:50.860 align:middle line:84%
So basically, again, we get
five significant digits correct.

00:07:50.860 --> 00:07:54.010 align:middle line:84%
How well is our
application parallelized?

00:07:54.010 --> 00:07:58.210 align:middle line:84%
One way to measure this is
to compute the speed up.

00:07:58.210 --> 00:08:02.560 align:middle line:84%
So we run our application with
one thread, with two threads,

00:08:02.560 --> 00:08:05.170 align:middle line:84%
with four threads,
et cetera, and we

00:08:05.170 --> 00:08:07.460 align:middle line:90%
record the running times.

00:08:07.460 --> 00:08:09.830 align:middle line:84%
Then we can compute
the speed up.

00:08:09.830 --> 00:08:13.120 align:middle line:84%
This is the time it takes
to execute the application

00:08:13.120 --> 00:08:16.660 align:middle line:84%
with a single thread
divided by the time it

00:08:16.660 --> 00:08:19.120 align:middle line:90%
takes with n threads.

00:08:19.120 --> 00:08:23.440 align:middle line:90%
So ideally, the speed up is n.

00:08:23.440 --> 00:08:25.840 align:middle line:84%
But as you can
see from the plot,

00:08:25.840 --> 00:08:28.480 align:middle line:84%
that is not the case
for this application.

00:08:28.480 --> 00:08:30.915 align:middle line:84%
For 12 threads,
we get a speed up

00:08:30.915 --> 00:08:34.840 align:middle line:84%
of 7, which is, of
course, not optimal

00:08:34.840 --> 00:08:38.409 align:middle line:90%
since we would hope for 12.

00:08:38.409 --> 00:08:41.049 align:middle line:84%
So there is room for
improvement here.

00:08:41.049 --> 00:08:44.500 align:middle line:84%
A better way to characterise
how well an application is

00:08:44.500 --> 00:08:48.220 align:middle line:84%
parallelized is through
the parallel efficiency.

00:08:48.220 --> 00:08:51.250 align:middle line:84%
Basically, that is
the speed up divided

00:08:51.250 --> 00:08:53.740 align:middle line:90%
by the number of threads.

00:08:53.740 --> 00:08:57.350 align:middle line:84%
Of course, for one thread,
the parallel efficiency

00:08:57.350 --> 00:08:59.000 align:middle line:90%
is going to be 1.

00:08:59.000 --> 00:09:02.430 align:middle line:84%
And then it typically
slowly decreases.

00:09:02.430 --> 00:09:05.180 align:middle line:84%
And as you can see
here, for 12 threads,

00:09:05.180 --> 00:09:07.410 align:middle line:84%
we end up with a
parallel efficiency

00:09:07.410 --> 00:09:13.130 align:middle line:90%
which is just below 0.6 or 60%.

00:09:13.130 --> 00:09:14.900 align:middle line:90%
Looks easy, right?

00:09:14.900 --> 00:09:19.080 align:middle line:84%
But keep in mind that this
was a really simple example.

00:09:19.080 --> 00:09:23.490 align:middle line:84%
So for more sophisticated
code, OpenMP is not that easy.

00:09:23.490 --> 00:09:27.800 align:middle line:84%
It requires quite a lot of work
to get your code efficient.

00:09:27.800 --> 00:09:29.750 align:middle line:84%
And also, you have
to be quite careful

00:09:29.750 --> 00:09:32.690 align:middle line:84%
not to introduce bugs
like race conditions

00:09:32.690 --> 00:09:35.470 align:middle line:90%
into your application.

00:09:35.470 --> 00:09:36.000 align:middle line:90%